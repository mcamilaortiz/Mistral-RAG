{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e561e1b6",
   "metadata": {},
   "source": [
    "# ALPACA-BOT SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad28d6f4",
   "metadata": {},
   "source": [
    "This solution was inspired by this github: https://gitlab.com/juliensimon/huggingface-demos/-/blob/main/langchain/rag-demo-sagemaker-textract/Building%20a%20simple%20RAG%20chatbot.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e7b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'sh'\n"
     ]
    }
   ],
   "source": [
    "# Instaling packages\n",
    "%%sh\n",
    "pip install --upgrade langchain==0.1.11\n",
    "pip install --upgrade langchain-core==0.1.30\n",
    "pip install SQLAlchemy\n",
    "pip install sagemaker amazon-textract-caller amazon-textract-textractor sentence-transformers pypdf pip install faiss-cpu -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91613d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import boto3, json, sagemaker\n",
    "from typing import Dict\n",
    "from langchain import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a982a15",
   "metadata": {},
   "source": [
    "\n",
    "**Why to use MISTRAL 7B?** It stands out for its efficiency despite being a smaller model compared to others, such as Llama. Despite its compact size, MISTRAL has demonstrated outstanding performance and additionally, its status as an open-source project provides users with the freedom to adapt and enhance the model according to their specific needs. The comprehensive documentation available facilitates its implementation and understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ff459",
   "metadata": {},
   "source": [
    "## Deploy LLM on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c025e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from HugginFace Mistral documentation\n",
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "\t'SM_NUM_GPUS': json.dumps(1)\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.4.2\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "  )\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbbb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting endpoint\n",
    "endpoint_name = predictor.endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3547b7",
   "metadata": {},
   "source": [
    "## Configure LLM in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd696a3",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "**max_new_tokens**: This parameter specifies the maximum number of new tokens (text units, such as words or subwords) that the model will generate as output. Limiting this quantity can be useful to control the length of the generated output.\n",
    "\n",
    "**top_p**: Also known as \"nucleus sampling\" or \"nucleus decoding,\" this parameter controls the cumulative probability of generated words. It limits generation to words that have a cumulative probability above a certain threshold (top_p). This helps prevent the model from continually generating improbable or irrelevant text.\n",
    "\n",
    "**temperature**: This parameter affects the smoothness of probability distributions during generation. A higher value of temperature (e.g., 1.0) makes the generation more diverse but potentially less coherent, while a lower value (e.g., 0.8) tends to produce more deterministic and coherent outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"max_new_tokens\": 512, \"top_p\": 0.8, \"temperature\": 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29135c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class and its methods facilitate the preparation and processing of JSON-formatted data for interaction with the model.\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps(\n",
    "            # Mistral prompt, see https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
    "            {\"inputs\": f\"<s>[INST] {prompt} [/INST]\", \"parameters\": {**model_kwargs}}\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        splits = response_json[0][\"generated_text\"].split(\"[/INST] \")\n",
    "        return splits[1]\n",
    "\n",
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is setting up a connection to a SageMaker endpoint using the AWS SDK for Python (Boto3) and the class named SagemakerEndpoint\n",
    "sm_client = boto3.client(\"sagemaker-runtime\") # needed for AWS credentials\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    content_handler=content_handler,\n",
    "    client=sm_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45079dc5",
   "metadata": {},
   "source": [
    "## RAG process with md files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library to process markdown documents\n",
    "from markdown2 import markdown_path\n",
    "\n",
    "class MarkdownLoader:\n",
    "    def load_document(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            markdown_content = file.read()\n",
    "        return markdown_content\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b64b5bc",
   "metadata": {},
   "source": [
    "### Uploading files to an S3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ba6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 bucket and prefix for markdown storage\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"clementine-documentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of S3 URIs\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "objs = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "objs = objs['Contents']\n",
    "uris = [f's3://{bucket}/{obj[\"Key\"]}' for obj in objs]\n",
    "uris    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07745175",
   "metadata": {},
   "source": [
    "### Analyze documents with Amazon Textract and split them in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the markdown files into chunks of a 256 size\n",
    "%%time\n",
    "\n",
    "\n",
    "markdown_loader = MarkdownLoader()  \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for uri in uris:\n",
    "    document = markdown_loader.load_document(uri)  \n",
    "    chunks = splitter.split_documents(document)\n",
    "    all_chunks += chunks\n",
    "    print(f\"Loaded {uri}, {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c74a8",
   "metadata": {},
   "source": [
    "### Embed document chunks and store them with CHROMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding model\n",
    "\n",
    "embedding_model_id = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the database\n",
    "%%time\n",
    "# Embed chunks\n",
    "embeddings_db = Chroma.from_documents(all_chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa22850",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d4d47",
   "metadata": {},
   "source": [
    "### Configure RAG using chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = embeddings_db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1db22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "prompt_template = \"\"\"\n",
    "Clementine: please answer the question below, focusing on numerical data and using only the context below.\n",
    "Don't invent facts. If you can't provide a factual answer, say you don't know what the answer is.\n",
    "\n",
    "question: {question}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ef004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Chatbot\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, \n",
    "    chain_type_kwargs = {\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06143a",
   "metadata": {},
   "source": [
    "### Asking the question again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64223366",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are all regions where AWS Sagemaker is available?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a47bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \" How to check if an endpoint is KMS encrypted?\"\n",
    "answer = chain.run({\"query\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d4337",
   "metadata": {},
   "source": [
    "## Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066351f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
